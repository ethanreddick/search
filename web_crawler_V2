import requests # Used to fetch the HTML content of a page (GET,POST,DELETE)
from bs4 import BeautifulSoup # Library for parsing HTML documents
import psycopg2 # PostgreSQL database adapter for Python
from queue import Queue # Python module that provides a queue implementation
import threading # Python module for working with threads
from urllib.parse import urljoin, urlparse # Allows us to convert all URLs to absolute URLs and parse URLs
from bloom_filter2 import BloomFilter # https://github.com/remram44/python-bloom-filter
import csv # Python's built-in module for processing .csv files

# TODO: Add multithreading for scraping

# CSV File containing the top 1 million most popular top-level-domains https://tranco-list.eu/list/25G99/1000000
TLD_CSV_FILE = './top_million_domains.csv'
acceptable_domain_extensions = [".com", ".net", ".edu", ".org", ".gov", ".mil"]
# Initialize bloom filter with 100M element capactiy and 1% error tolerance
bloom_filter = BloomFilter(max_elements = 100000000, error_rate = 0.01)
crawled_sites_count = 0
skipped_sites_count = 0

# Database connection setup
def connect_database():
    return psycopg2.connect(
        host = "localhost",
        database = "scraper_db",
        user = "ethan",
        password = "search"
    )

# Collect top-level domains (TLDs) from the .csv file
def read_domains_from_csv(file_name):
    with open(file_name, newline = '') as csvfile:
        reader = csv.reader(csvfile)
        # In the .csv file the domain names are the second entry in each row (second column)
        # Example format of the .csv file is '1,google.com' on the first line
        return [row[1] for row in reader] # This returns each top-level domain name

# Saves page URL, title, and headers to the page_details table in DB
def save_page_details(url, title, headers):
    conn = connect_database()
    cur = conn.cursor()
    # Ensuring all URLs can fit in the DB
    if len(url) > 254: return None
    try:
        cur.execute(
            """
            INSERT INTO page_details (url, title, headers) 
            VALUES (%s, %s, %s) ON CONFLICT (url) DO NOTHING RETURNING id
            """,
            (url, title, headers)
        )
        page_id = cur.fetchone()  # Used so save_page_links can associate links with this page
        conn.commit()
        # Ensure the function either returns the correct page_id or 'None'
        return page_id[0] if page_id else None
    except psycopg2.DatabaseError as e:
        print(f"Database error: {e}")
        return None
    finally:
        cur.close()
        conn.close()
        return page_id

# Saves page links to the page_links table in DB
def save_page_links(page_id, links):
    conn = connect_database()
    cur = conn.cursor()

    for link in links:
        # Ensuring all link entries can fit in the DB
        if len(link) > 254: continue
        cur.execute(
            """
            INSERT INTO page_links (page_id, link) 
            VALUES (%s, %s) ON CONFLICT DO NOTHING
            """,
            (page_id, link)
        )
    conn.commit()
    cur.close()
    conn.close()

# Attempts to determine if a site is of high quality or if it should be discarded, returns true for quality.
def evaluate_site_quality(url):
    global acceptable_domain_extensions

    # All rules that can cause a site to fail prior to visiting the site
    # Eliminate invalid URLs not starting with 'https://'
    if not url.startswith("https://"): return False
    # Eliminate URLs that do not have an acceptable domain extension
    if not any(extension in url for extension in acceptable_domain_extensions): return False

    # All other rules
    response = requests.get(url)
    # Eliminate sites with a response time above 1 second
    if response.elapsed.total_seconds() > 1: return False
    # Eliminate sites with a response other than an HTML document
    if "text/html" not in response.headers.get('Content-Type', ''): return False

    return True

# Checks to see if a URL belongs to the same top-level domain
def is_same_domain(url, domain):
    # Parses the url, separates into parts, and checks if the 'network location'
    # or netloc section ends with the given domain name.
    return urlparse(url).netloc.endswith(domain)

# Visits a page and collects the title, headers, and links, and passes them to the DB querying functions.
def scrape_page(domain):
    global crawled_sites_count
    global skipped_sites_count

    # Initializing a set to store the subdomain queue for this domain
    urls_to_visit = {f"https://{domain}"}

    # For all URLs remaining in the queue
    while urls_to_visit:
        # Pop top URL off the stack
        url = urls_to_visit.pop()
        # Check each URL against the bloom filter to make sure it hasn't been seen before
        # Also checking to ensure only subdomains are crawled
        if url in bloom_filter or not is_same_domain(url, domain):
            skipped_sites_count += 1
            continue
        # Add the URL to the bloom filter
        bloom_filter.add(url)

        # Only check subdomains that pass the quality check
        if evaluate_site_quality(url):
            try:
                response = requests.get(url) # Sends an HTTP GET request using 'requests'
                soup = BeautifulSoup(response.content, 'html.parser') # Creates the BeautifulSoup object which represents the page document as a nested data structure

                # Getting the title, links, and headers using the BeautifulSoup object
                # Getting the page title if it exists, otherwise setting it to ""
                page_title = soup.title.string if soup.title else ""
                
                # Grabbing all headers, getting their text, and joining them using a space as the delimiter
                headers = ' '.join([header.get_text() 
                for header in soup.find_all(
                    ['h1', 'h2', 'h3']
                    )])
                
                # Collects the links and converts them to absolute URLs using urljoin.
                # Finds all '<a>' anchor elements in the BeautifulSoup object that contain an 'href'
                # attribute (which typically contains a link/URL). Then for each urljoin is used to
                # convert it to an absolute URL, and these are stored in the list 'links'
                links = [
                    urljoin(url, a['href'])
                    for a in soup.find_all(
                        'a', href = True
                        )
                    ]

                # Saves page URL, title, and headers to page_details table in DB
                page_id = save_page_details(url, page_title, headers)
                # Saves page links to page_links table in DB as long as a page_id was returned
                if page_id is not None:
                    save_page_links(page_id, links)
                
                # Debug statement that provides updates
                crawled_sites_count += 1
                skipped_percentage = (skipped_sites_count / crawled_sites_count) * 100
                print("Pages crawled:", crawled_sites_count, "Skipped:", skipped_sites_count, f"({skipped_percentage :.2f}% Skipped)")
                
                # Only add new subdomain URLs to the queue
                for link in links:
                    if link not in bloom_filter:
                        urls_to_visit.add(link)

            except requests.RequestException as e:
                print(f"Error fetching {url}: {e}")
        else:
            skipped_sites_count += 1

    return

def main():
    global TLD_CSV_FILE
    global skipped_sites_count

    # Initializing the list of top-level domains from the .csv file
    domains_to_visit = read_domains_from_csv(TLD_CSV_FILE)
    print("Finished reading .csv file.")

    for domain in domains_to_visit:
        scrape_page(domain)

main()
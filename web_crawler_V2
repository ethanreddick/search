import requests # Used to fetch the HTML content of a page (GET,POST,DELETE)
from bs4 import BeautifulSoup # Library for parsing HTML documents
import psycopg2 # PostgreSQL database adapter for Python
from queue import Queue # Python module that provides a queue implementation
import threading # Python module for working with threads
from urllib.parse import urljoin # Allows us to convert all URLs to absolute URLs
from bloom_filter2 import BloomFilter # https://github.com/remram44/python-bloom-filter

SEED_URL = "https://www.scrapingbee.com/blog/crawling-python/"
acceptable_domain_extensions = [".com", ".net", ".edu", ".org", ".gov", ".mil"]
# Initialize bloom filter with 100M element capactiy and 1% error tolerance
bloom_filter = BloomFilter(max_elements = 100000000, error_rate = 0.01)
crawled_sites_count = 0
skipped_sites_count = 0

# Database connection setup
def connect_database():
    return psycopg2.connect(
        host = "localhost",
        database = "scraper_db",
        user = "ethan",
        password = "search"
    )

# Saves page URL, title, and headers to the page_details table in DB
def save_page_details(url, title, headers):
    conn = connect_database()
    cur = conn.cursor()
    # Ensuring all URLs can fit in the DB
    if len(url) > 254: return None
    try:
        cur.execute(
            """
            INSERT INTO page_details (url, title, headers) 
            VALUES (%s, %s, %s) ON CONFLICT (url) DO NOTHING RETURNING id
            """,
            (url, title, headers)
        )
        page_id = cur.fetchone()  # Used so save_page_links can associate links with this page
        conn.commit()
        # Ensure the function either returns the correct page_id or 'None'
        return page_id[0] if page_id else None
    except psycopg2.DatabaseError as e:
        print(f"Database error: {e}")
        return None
    finally:
        cur.close()
        conn.close()
        return page_id

# Saves page links to the page_links table in DB
def save_page_links(page_id, links):
    conn = connect_database()
    cur = conn.cursor()

    for link in links:
        # Ensuring all link entries can fit in the DB
        if len(link) > 254: continue
        cur.execute(
            """
            INSERT INTO page_links (page_id, link) 
            VALUES (%s, %s) ON CONFLICT DO NOTHING
            """,
            (page_id, link)
        )
    conn.commit()
    cur.close()
    conn.close()

# Attempts to determine if a site is of high quality or if it should be discarded, returns true for quality.
def evaluate_site_quality(url):
    global acceptable_domain_extensions
    # All rules that can cause a site to fail prior to visiting the site
    # Eliminate invalid URLs not starting with 'https://'
    if not url.startswith("https://"): return False
    # Eliminate URLs that do not have an acceptable domain extension
    if not any(extension in url for extension in acceptable_domain_extensions): return False

    # All other rules
    response = requests.get(url)
    # Eliminate sites with a response time above 1 second
    if response.elapsed.total_seconds() > 1: return False

    return True

# Visits a page and collects the title, headers, and links, and passes them to the DB querying functions.
def scrape_page(url):
    global crawled_sites_count
    global skipped_sites_count
    response = requests.get(url) # Sends an HTTP GET request using 'requests'
    soup = BeautifulSoup(response.content, 'html.parser') # Creates the BeautifulSoup object which represents the page document as a nested data structure

    # Getting the title, links, and headers using the BeautifulSoup object
    # Getting the page title if it exists, otherwise setting it to ""
    page_title = soup.title.string if soup.title else ""
    
    # Grabbing all headers, getting their text, and joining them using a space as the delimiter
    headers = ' '.join([header.get_text() 
    for header in soup.find_all(
        ['h1', 'h2', 'h3']
        )])
    
    # Collects the links and converts them to absolute URLs using urljoin.
    # Finds all '<a>' anchor elements in the BeautifulSoup object that contain an 'href'
    # attribute (which typically contains a link/URL). Then for each urljoin is used to
    # convert it to an absolute URL, and these are stored in the list 'links'
    links = [
        urljoin(url, a['href'])
        for a in soup.find_all(
            'a', href = True
            )
        ]

    # Saves page URL, title, and headers to page_details table in DB
    page_id = save_page_details(url, page_title, headers)
    # Saves page links to page_links table in DB as long as a page_id was returned
    if page_id is not None:
        save_page_links(page_id, links)

    crawled_sites_count += 1
    print("Pages crawled:", crawled_sites_count, "Skipped:", skipped_sites_count)
    return links

def main(seed_url):
    global skipped_sites_count
    # Initializing the URLs in the queue with the SEED_URL
    urls_to_visit = set([seed_url])

    # For all URLs remaining in the queue
    while urls_to_visit:
        # Pop top URL off the stack
        current_url = urls_to_visit.pop()
        # Check each URL against the bloom filter to make sure it hasn't been seen before
        if current_url in bloom_filter:
            continue
        # Add the URL to the bloom filter
        bloom_filter.add(current_url)

        # Skip a site if it fails the quality check
        if evaluate_site_quality(current_url):
            # Grab links and store page details, add it to the visited list, and repeat
            new_links = scrape_page(current_url)

            # For all newly discovered links, check all against the bloom filter
            for link in new_links:
                if link not in bloom_filter:
                    urls_to_visit.add(link)
        else:
            skipped_sites_count += 1
            
# Main execution starting from SEED_URL
main(SEED_URL)
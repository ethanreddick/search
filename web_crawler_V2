import requests # Used to fetch the HTML content of a page (GET,POST,DELETE)
from bs4 import BeautifulSoup # Library for parsing HTML documents
import psycopg2 # PostgreSQL database adapter for Python
from queue import Queue # Python module that provides a queue implementation
import threading # Python module for working with threads
from urllib.parse import urljoin # Allows us to convert all URLs to absolute URLs

SEED_URL = "https://www.scrapingbee.com/blog/crawling-python/"

# Database connection setup
def connect_database():
    return psycopg2.connect(
        host = "localhost",
        database = "scraper_db",
        user = "ethan",
        password = "search"
    )

# Saves page URL, title, and headers to the page_details table in DB
def save_page_details(url, title, headers):
    conn = connect_database()
    cur = conn.cursor()
    try:
        cur.execute(
            """
            INSERT INTO page_details (url, title, headers) 
            VALUES (%s, %s, %s) ON CONFLICT (url) DO NOTHING RETURNING id
            """,
            (url, title, headers)
        )
        page_id = cur.fetchone()  # Used so save_page_links can associate links with this page
        conn.commit()
        # Ensure the function either returns the correct page_id or 'None'
        return page_id[0] if page_id else None
    except psycopg2.DatabaseError as e:
        print(f"Database error: {e}")
        return None
    finally:
        cur.close()
        conn.close()
        return page_id

# Saves page links to the page_links table in DB
def save_page_links(page_id, links):
    conn = connect_database()
    cur = conn.cursor()

    for link in links:
        cur.execute(
            """
            INSERT INTO page_links (page_id, link) 
            VALUES (%s, %s) ON CONFLICT DO NOTHING
            """,
            (page_id, link)
        )
    conn.commit()
    cur.close()
    conn.close()

# Visits a page and collects the title, headers, and links, and passes them to the DB querying functions.
def scrape_page(url):
    response = requests.get(url) # Sends an HTTP GET request using 'requests'
    soup = BeautifulSoup(response.content, 'html.parser') # Creates the BeautifulSoup object which represents the page document as a nested data structure

    # Getting the title, links, and headers using the BeautifulSoup object
    # Getting the page title if it exists, otherwise setting it to ""
    page_title = soup.title.string if soup.title else ""
    
    # Grabbing all headers, getting their text, and joining them using a space as the delimiter
    headers = ' '.join([header.get_text() 
    for header in soup.find_all(
        ['h1', 'h2', 'h3']
        )])
    
    # Collects the links and converts them to absolute URLs using urljoin.
    # Finds all '<a>' anchor elements in the BeautifulSoup object that contain an 'href'
    # attribute (which typically contains a link/URL). Then for each urljoin is used to
    # convert it to an absolute URL, and these are stored in the list 'links'
    links = [
        urljoin(url, a['href'])
        for a in soup.find_all(
            'a', href = True
            )
        ]

    print("Adding links...\n%s", links) # Debug statement to check process

    # Saves page URL, title, and headers to page_details table in DB
    page_id = save_page_details(url, page_title, headers)
    # Saves page links to page_links table in DB as long as a page_id was returned
    if page_id is not None:
        save_page_links(page_id, links)

    return links

def main(seed_url):
    # Tracking all visited URLs. TODO: Is this type of checking sustainable at scale?
    visited_urls = set()
    # Initializing the URLs in the queue with the SEED_URL
    urls_to_visit = set([seed_url])

    # For all URLs remaining in the queue
    while urls_to_visit:
        # Pop top URL off the stack
        current_url = urls_to_visit.pop()
        if current_url not in visited_urls:
            # Grab links and store page details, add it to the visited list, and repeat
            new_links = scrape_page(current_url)
            visited_urls.add(current_url)
            urls_to_visit.update(set(new_links) - visited_urls)

# Main execution starting from SEED_URL
main(SEED_URL)